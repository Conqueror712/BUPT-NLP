å§“åï¼šå·©ç¾½é£ä¸¨å­¦å·ï¼š2021522077ä¸¨ç­çº§ï¼š2021669101ä¸¨è¯¾ç¨‹ï¼šè‡ªç„¶è¯­è¨€å¤„ç†

## å®éªŒæŠ¥å‘Šï¼šåŸºäºTransformerçš„å‘½åå®ä½“è¯†åˆ«

### 1. æ ‡ç­¾é›†è¯´æ˜

é€šè¿‡ `extract_labels.py` å¯¹æ ‡ç­¾è¿›è¡Œæå–ï¼Œå¾—åˆ°ä»¥ä¸‹ç§ç±»çš„æ ‡ç­¾ï¼š

- `I_LOC`ï¼šåœ°ç‚¹å®ä½“çš„å†…éƒ¨æ ‡è®°ï¼ˆInside Locationï¼‰ï¼Œç”¨äºæ ‡è¯†åœ°ç‚¹å®ä½“çš„åç»­éƒ¨åˆ†ã€‚
- `B_PER`ï¼šäººåå®ä½“çš„å¼€å§‹æ ‡è®°ï¼ˆBegin Personï¼‰ï¼Œç”¨äºæ ‡è¯†äººåå®ä½“çš„èµ·å§‹éƒ¨åˆ†ã€‚
- `B_ORG`ï¼šç»„ç»‡åå®ä½“çš„å¼€å§‹æ ‡è®°ï¼ˆBegin Organizationï¼‰ï¼Œç”¨äºæ ‡è¯†ç»„ç»‡åå®ä½“çš„èµ·å§‹éƒ¨åˆ†ã€‚
- `I_ORG`ï¼šç»„ç»‡åå®ä½“çš„å†…éƒ¨æ ‡è®°ï¼ˆInside Organizationï¼‰ï¼Œç”¨äºæ ‡è¯†ç»„ç»‡åå®ä½“çš„åç»­éƒ¨åˆ†ã€‚
- `I_PER`ï¼šäººåå®ä½“çš„å†…éƒ¨æ ‡è®°ï¼ˆInside Personï¼‰ï¼Œç”¨äºæ ‡è¯†äººåå®ä½“çš„åç»­éƒ¨åˆ†ã€‚
- `B_LOC`ï¼šåœ°ç‚¹å®ä½“çš„å¼€å§‹æ ‡è®°ï¼ˆBegin Locationï¼‰ï¼Œç”¨äºæ ‡è¯†åœ°ç‚¹å®ä½“çš„èµ·å§‹éƒ¨åˆ†ã€‚
- `O`ï¼šéå‘½åå®ä½“çš„æ ‡è®°ï¼ˆOutsideï¼‰ï¼Œç”¨äºæ ‡è¯†éå‘½åå®ä½“çš„éƒ¨åˆ†ï¼Œæˆ–è€…ä¸å±äºä»»ä½•ç‰¹å®šå®ä½“ç±»åˆ«çš„æ–‡æœ¬ã€‚
- `B_T`ï¼šæ—¶é—´å®ä½“çš„å¼€å§‹æ ‡è®°ï¼ˆBegin Timeï¼‰ï¼Œç”¨äºæ ‡è¯†æ—¶é—´å®ä½“çš„èµ·å§‹éƒ¨åˆ†ã€‚
- `I_T`ï¼šæ—¶é—´å®ä½“çš„å†…éƒ¨æ ‡è®°ï¼ˆInside Timeï¼‰ï¼Œç”¨äºæ ‡è¯†æ—¶é—´å®ä½“çš„åç»­éƒ¨åˆ†ã€‚

æ­¤æ ‡ç­¾é›†ä¸­çš„è¿™äº›æ ‡ç­¾é€šå¸¸åœ¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­ä½¿ç”¨ï¼Œç”¨äºå¯¹ç»™å®šçš„æ–‡æœ¬è¿›è¡Œå®ä½“è¯†åˆ«å’Œåˆ†ç±»ã€‚

### 2. æ¨¡å‹ç»†èŠ‚è¯´æ˜

BERT-CRFæ¨¡å‹æ˜¯ä¸€ç§å°†BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰ä¸CRFï¼ˆConditional Random Fieldsï¼‰ç»“åˆï¼Œç”¨äºåºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡çš„å¼ºå¤§æ¨¡å‹ï¼Œä»¥ä¸‹æ˜¯å¯¹BERT-CRFæ¨¡å‹çš„ç®€å•ä»‹ç»ã€‚

#### 2.1 æ¨¡å‹ç»“æ„åŠå…¶ç»“æ„å‚æ•°

##### 2.1.1 BERT-CRFæ¨¡å‹
- **BERTæ¨¡å‹**ï¼šBERTæ˜¯ç”±Googleæå‡ºçš„ä¸€ç§é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºæ¨¡å‹ï¼Œå®ƒé‡‡ç”¨åŒå‘Transformeræ¶æ„ã€‚BERTçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ æ–‡æœ¬çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚BERTçš„é¢„è®­ç»ƒä»»åŠ¡åŒ…æ‹¬ï¼š
  
  1. **Masked Language Model (MLM)**ï¼šéšæœºé®æ©è¾“å…¥åºåˆ—ä¸­çš„ä¸€äº›å•è¯ï¼Œç„¶åè®©æ¨¡å‹é¢„æµ‹è¿™äº›è¢«é®æ©çš„å•è¯ã€‚
  2. **Next Sentence Prediction (NSP)**ï¼šé¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦ç´§é‚»ã€‚
  
  BERTçš„è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªè¾“å…¥tokenè¡¨ç¤ºçš„åºåˆ—ï¼Œè¿™äº›è¡¨ç¤ºåŒ…å«äº†ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…¶ç¼–ç å¯ç®€å•è¡¨ç¤ºä¸ºï¼šç»™å®šä¸€ä¸ªè¾“å…¥åºåˆ— $ğ‘‹=[ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘›]$ï¼Œé€šè¿‡BERTç¼–ç åå¾—åˆ°éšå±‚è¡¨ç¤º $ğ»=[â„_1,â„_2,â€¦,â„_ğ‘›]$ï¼Œ$ğ»=BERT(ğ‘‹)$ã€‚
  
  åœ¨æœ¬å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¢„è®­ç»ƒçš„BERTæ¨¡å‹ `bert-base-chinese` ä½œä¸ºå…¶ç¼–ç å™¨ã€‚BERTæ¨¡å‹çš„ä¸»è¦ç»“æ„å‚æ•°åŒ…æ‹¬ï¼š
  
  - **Transformerå±‚æ•°**ï¼š12å±‚ï¼ˆå¯¹äº `bert-base` æ¨¡å‹ï¼‰
  - **éšè—å±‚å¤§å°**ï¼š768ç»´
  - **æ³¨æ„åŠ›å¤´æ•°**ï¼š12ä¸ª
  - **æœ€å¤§è¾“å…¥tokenæ•°**ï¼š512ä¸ª
  
- **å…¨è¿æ¥å±‚**ï¼šåœ¨BERTçš„è¾“å‡ºä¹‹åï¼Œä½¿ç”¨äº†ä¸€ä¸ªå…¨è¿æ¥å±‚å°†BERTçš„è¾“å‡ºæ˜ å°„åˆ°æ ‡ç­¾ç©ºé—´ï¼ˆè¾“å‡ºç»´æ•°ä¸ºæ ‡ç­¾çš„æ•°é‡ï¼‰ã€‚

- **CRFå±‚**ï¼šCRFæ˜¯ä¸€ç§ç”¨äºåºåˆ—æ ‡æ³¨çš„æ¦‚ç‡å›¾æ¨¡å‹ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†æ ‡ç­¾ä¹‹é—´å­˜åœ¨ä¾èµ–å…³ç³»çš„ä»»åŠ¡ã€‚CRFé€šè¿‡è€ƒè™‘æ ‡ç­¾åºåˆ—çš„å…¨å±€æœ€ä¼˜æ€§æ¥æé«˜æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚åœ¨BERT-CRFæ¨¡å‹ä¸­ï¼ŒBERTä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´è¡¨ç¤ºç©ºé—´ã€‚ç„¶åï¼Œå°†è¿™äº›è¡¨ç¤ºä½œä¸ºCRFå±‚çš„è¾“å…¥ï¼ŒCRFå±‚è´Ÿè´£å¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œåºåˆ—æ ‡æ³¨ã€‚åœ¨å…¨è¿æ¥å±‚ä¹‹åï¼Œä½¿ç”¨CRFå±‚è¿›è¡Œæ ‡ç­¾åºåˆ—è§£ç ï¼Œä»¥å»ºæ¨¡æ ‡ç­¾ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

    åœ¨CRFå±‚ï¼Œå®šä¹‰è½¬ç§»å¾—åˆ†çŸ©é˜µ $A$ï¼Œå…¶ä¸­ $A_{ij}$ è¡¨ç¤ºä»æ ‡ç­¾ $i$ è½¬ç§»åˆ°æ ‡ç­¾ $j$ çš„å¾—åˆ†ã€‚å®šä¹‰æ¯ä¸ªæ—¶é—´æ­¥çš„å¾—åˆ†å‡½æ•° $s(y_t | h_t)$ï¼Œè¡¨ç¤ºåœ¨æ—¶é—´æ­¥ $t$ å¤„ï¼Œè¾“å‡ºæ ‡ç­¾ä¸º $y_t$ çš„å¾—åˆ†ã€‚åºåˆ— $Y = [y_1, y_2, \ldots, y_n]$ çš„æ€»å¾—åˆ† $S$ å¯ä»¥è¡¨ç¤ºä¸ºï¼š

    $ S(X, Y) = \sum_{t=1}^{n} s(y_t | h_t) + \sum_{t=1}^{n-1} A_{y_t, y_{t+1}} $

    CRFå±‚çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ­£ç¡®æ ‡ç­¾åºåˆ—çš„å¾—åˆ†ï¼ŒåŒæ—¶æœ€å°åŒ–æ‰€æœ‰å¯èƒ½æ ‡ç­¾åºåˆ—çš„å¾—åˆ†ä¹‹å’Œã€‚é€šè¿‡softmaxæ¥è®¡ç®—æ¯ä¸ªæ ‡ç­¾åºåˆ—çš„æ¦‚ç‡ï¼š

    $ P(Y|X) = \frac{\exp(S(X, Y))}{\sum_{Y'} \exp(S(X, Y'))} $

    è€Œè®­ç»ƒè¿‡ç¨‹é€šè¿‡æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ä¼°è®¡æ¥è¿›è¡Œã€‚

##### 2.1.2 é¢„å¤„ç†éƒ¨åˆ†
- **åˆå§‹å­—å‘é‡**ï¼šä½¿ç”¨è…¾è®¯AIå®éªŒå®¤æä¾›çš„ä¸­æ–‡è¯å‘é‡æ–‡ä»¶ `tencent-ailab-embedding-zh-d100-v0.2.0-s.bin` ï¼Œè¿™äº›è¯å‘é‡æ˜¯åŸºäºWord2Vecè®­ç»ƒçš„ã€‚
  - **å‘é‡ç»´æ•°**ï¼š100ç»´

##### 2.1.3 ä¸ºä»€ä¹ˆBERT-CRFé€‚åˆNERä»»åŠ¡

1. **ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸°å¯Œ**ï¼šBERTçš„åŒå‘ç¼–ç èƒ½å¤Ÿæ•æ‰åˆ°æ–‡æœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™æ˜¯è¯†åˆ«å‘½åå®ä½“ï¼ˆå¦‚äººåã€åœ°åç­‰ï¼‰æ‰€å¿…éœ€çš„ï¼Œå› ä¸ºå®ä½“è¯†åˆ«é€šå¸¸ä¾èµ–äºä¸Šä¸‹æ–‡ã€‚
2. **æ ‡ç­¾ä¾èµ–å¤„ç†èƒ½åŠ›å¼º**ï¼šNERä»»åŠ¡ä¸­ï¼Œæ ‡ç­¾ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œ`B-PER` åé¢é€šå¸¸è·Ÿç€ `I-PER`ï¼‰ã€‚CRFå±‚é€šè¿‡å»ºæ¨¡æ ‡ç­¾ä¹‹é—´çš„è½¬ç§»æ¦‚ç‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰è¿™äº›ä¾èµ–å…³ç³»ï¼Œæé«˜æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚
3. **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šBERT-CRFæ¨¡å‹å¯ä»¥ç«¯åˆ°ç«¯è®­ç»ƒï¼Œä½¿å¾—ç‰¹å¾æå–å’Œåºåˆ—æ ‡æ³¨è¿‡ç¨‹æ— ç¼ç»“åˆï¼Œä¼˜åŒ–æ•ˆæœæ›´å¥½ã€‚
4. **å¤„ç†é•¿åºåˆ—èƒ½åŠ›**ï¼šBERTèƒ½å¤Ÿå¤„ç†é•¿æ–‡æœ¬åºåˆ—ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡åˆ†æ®µå¤„ç†ï¼ˆä½¿ç”¨ç‰¹æ®Šçš„ `[SEP]` æ ‡è®°ï¼‰æ¥æœ‰æ•ˆåœ°å¤„ç†è¶…é•¿æ–‡æœ¬ã€‚

#### 2.2 æ•°æ®å¤„ç†

##### 2.2.1 æ•°æ®é›†
- **`NerDataset` ç±»**ï¼šå®šä¹‰äº†NERä»»åŠ¡çš„æ•°æ®é›†ç±»ï¼Œä½¿ç”¨BERTçš„tokenizerå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œtokenizationï¼Œå¹¶å°†æ ‡ç­¾è½¬åŒ–ä¸ºå¼ é‡ã€‚
- **`padding_collate` å‡½æ•°**ï¼šè‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è¿›è¡Œpaddingã€‚

#### 2.3 è®­ç»ƒéƒ¨åˆ†

##### 2.3.1 è®­ç»ƒå‚æ•°
- **è®­ç»ƒç®—æ³•**ï¼šä½¿ç”¨Adamä¼˜åŒ–å™¨ `torch.optim.Adam`ã€‚
- **å­¦ä¹ ç‡**ï¼š0.001
- **è®­ç»ƒæ‰¹æ¬¡å¤§å°**ï¼š512
- **è®­ç»ƒè½®æ•°**ï¼š3è½®

##### 2.3.2 è®­ç»ƒæµç¨‹
- **æ•°æ®åŠ è½½**ï¼šé€šè¿‡ `DataLoader` åŠ è½½è®­ç»ƒå’Œå¼€å‘æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨è‡ªå®šä¹‰çš„collateå‡½æ•°è¿›è¡Œpaddingã€‚
- **æ¨¡å‹è®­ç»ƒ**ï¼šåœ¨æ¯ä¸ªè®­ç»ƒè½®æ¬¡ä¸­ï¼Œæ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­å¹¶æ›´æ–°å‚æ•°ã€‚
  - **æŸå¤±è®¡ç®—**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨CRFå±‚çš„å‰å‘ç®—æ³•è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ã€‚
- **æ¨¡å‹è¯„ä¼°**ï¼šåœ¨å¼€å‘é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—F1åˆ†æ•°ã€‚

#### 2.4 é¢„æµ‹éƒ¨åˆ†

##### 2.4.1 é¢„æµ‹æµç¨‹
- **æ•°æ®é¢„å¤„ç†**ï¼šå°†æµ‹è¯•é›†çš„æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†å’Œtokenizationã€‚
- **æ¨¡å‹é¢„æµ‹**ï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹å¹¶è§£ç å¾—åˆ°æ ‡ç­¾åºåˆ—ã€‚
- **ç»“æœä¿å­˜**ï¼šå°†é¢„æµ‹çš„æ ‡ç­¾ç»“æœä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚

#### 2.5 å¯è§†åŒ–
- **æŸå¤±æ›²çº¿**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•å¹¶ç»˜åˆ¶æŸå¤±æ›²çº¿ï¼Œå…·ä½“è§ä¸‹æ–‡ã€‚
- **F1åˆ†æ•°æ›²çº¿**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•å¹¶ç»˜åˆ¶F1åˆ†æ•°æ›²çº¿ï¼Œå…·ä½“è§ä¸‹æ–‡ã€‚

#### 2.6 ä»£ç ç¤ºä¾‹
ä»¥ä¸‹æ˜¯ä»£ç çš„ä¸»è¦éƒ¨åˆ†ç¤ºä¾‹ï¼š

```python
class BertCrf(nn.Module):
    def __init__(self, output_size, tokenizer, bert_model_name='bert-base-chinese'):
        super(BertCrf, self).__init__()
        self.tokenizer = tokenizer
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.hidden_size = self.bert.config.hidden_size
        self.fc = nn.Linear(self.hidden_size, output_size)
        self.crf = CRF(output_size)

    def forward(self, inputs, target, is_predict=False):
        bert_output = self.bert(**inputs).last_hidden_state
        logits = self.fc(bert_output)
        if not is_predict:
            mask = inputs['attention_mask'].bool()
            loss = -self.crf.forward(logits, target, mask=mask)
            return loss
        else:
            mask = inputs['attention_mask'].bool()
            decode = self.crf.viterbi_decode(logits, mask=mask)
            return decode
```

è®­ç»ƒå’Œé¢„æµ‹çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
def train(max_epoch, batch_size, bert_model_path, save_path='model/model_bert.pth'):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    pre = Preprocess(bert_model_path=bert_model_path)
    tokenizer = pre.tokenizer
    train_corpus, train_tags = pre.preprocess_corpus('train.txt', 'train_TAG.txt', has_tag=True)
    dev_corpus, dev_tags = pre.preprocess_corpus('dev.txt', 'dev_TAG.txt', has_tag=True)
    train_dataset = NerDataset(train_corpus, train_tags, tokenizer)
    dev_dataset = NerDataset(dev_corpus, dev_tags, tokenizer)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=padding_collate, shuffle=True)
    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=padding_collate, shuffle=True)

    model = BertCrf(output_size=len(pre.tag2id), tokenizer=tokenizer).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(max_epoch):
        print(f'Epoch {epoch + 1}/{max_epoch}')
        model.train()
        for i, (inputs, labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):
            inputs = {key: val.to(device) for key, val in inputs.items()}
            labels = labels.to(device)
            optimizer.zero_grad()
            loss = model(inputs, labels)
            loss.backward()
            optimizer.step()
```

### 3. æŒ‡æ ‡ç»Ÿè®¡

<img src="./result/loss.jpg" alt="image" style="zoom:33%;" /> <img src="./result/F1_v2.jpg" alt="image" style="zoom:33%;" /> <img src="./result/F1.jpg" alt="image" style="zoom:33%;" />

ç»¼ä¸Šï¼Œæœ¬å®éªŒè®­ç»ƒäº†7ä¸ªEpochï¼Œåœ¨éªŒè¯é›†ä¸Šçš„lossè¾¾åˆ°0.01ï¼ŒåŒæ—¶F1åˆ†æ•°è¾¾åˆ°94.32ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼ŒF1å…ˆå‡é«˜ï¼Œåˆ°Epoch7ä¹‹ååˆé™ä½ï¼ŒåŒæ—¶lossä¹Ÿå…ˆé™ä½å†å‡é«˜ï¼›æ­¤å¤–ï¼Œå½“ç»§ç»­è¿›è¡Œè®­ç»ƒåˆ°Epoch8çš„æ—¶å€™ï¼ŒF1å¼€å§‹å›å‡ï¼Œå¹¶é€æ¸è¶…è¿‡åŸæ¥çš„æœ€é«˜å€¼ï¼Œå¯èƒ½çš„åŸå› å¦‚ä¸‹ï¼š

#### 3.1 è¿‡æ‹Ÿåˆ

- **ç°è±¡**ï¼šåœ¨è®­ç»ƒåˆæœŸï¼Œæ¨¡å‹çš„F1å¾—åˆ†ä¸Šå‡ï¼ŒLossä¸‹é™ï¼Œè¡¨æ˜æ¨¡å‹æ­£åœ¨å­¦ä¹ æ•°æ®çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œåˆ°è¾¾æŸä¸ªç‚¹åï¼ŒF1å¾—åˆ†å¼€å§‹ä¸‹é™ï¼Œè€ŒLosså¼€å§‹ä¸Šå‡ï¼Œè¡¨æ˜æ¨¡å‹å¯èƒ½å¼€å§‹è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
- **è§£é‡Š**ï¼šè¿‡æ‹Ÿåˆå‘ç”Ÿåœ¨æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè¡¨ç°è¾ƒå·®ã€‚æ­¤æ—¶ï¼Œæ¨¡å‹å·²ç»å¼€å§‹è®°ä½è®­ç»ƒæ•°æ®ä¸­çš„å™ªéŸ³å’Œç»†èŠ‚ï¼Œè€Œä¸æ˜¯å­¦ä¹ åˆ°æ³›åŒ–çš„ç‰¹å¾ã€‚
- **è§£å†³æ–¹æ¡ˆ**ï¼š
  - **å¢åŠ æ­£åˆ™åŒ–**ï¼šä½¿ç”¨L2æ­£åˆ™åŒ–æˆ–Dropoutå±‚æ¥å‡å°‘æ¨¡å‹çš„è¿‡æ‹Ÿåˆã€‚
  - **æ•°æ®å¢å¼º**ï¼šé€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯ç”Ÿæˆæ›´å¤šçš„è®­ç»ƒæ ·æœ¬ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
  - **æ—©åœ**ï¼šç›‘æ§éªŒè¯é›†çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ€§èƒ½å¼€å§‹ä¸‹é™æ—¶æå‰åœæ­¢è®­ç»ƒã€‚
  - **å‡å°æ¨¡å‹å¤æ‚åº¦**ï¼šå‡å°‘æ¨¡å‹çš„å‚æ•°æ•°é‡ï¼Œé™ä½æ¨¡å‹çš„å¤æ‚åº¦ã€‚

#### 3.2 å­¦ä¹ ç‡é—®é¢˜

- **ç°è±¡**ï¼šå¦‚æœå­¦ä¹ ç‡è®¾ç½®ä¸åˆé€‚ï¼Œå¯èƒ½å¯¼è‡´åœ¨æŸäº›Epochä¸­æ¨¡å‹çš„æ€§èƒ½æ³¢åŠ¨ã€‚æ¯”å¦‚ï¼Œå­¦ä¹ ç‡å¤ªé«˜ï¼Œæ¨¡å‹çš„å‚æ•°æ›´æ–°è¿‡å¤§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­éœ‡è¡ï¼Œæ€§èƒ½æ³¢åŠ¨è¾ƒå¤§ã€‚
- **è§£é‡Š**ï¼šå­¦ä¹ ç‡çš„è®¾ç½®å¯¹è®­ç»ƒè¿‡ç¨‹æœ‰å¾ˆå¤§å½±å“ã€‚è¾ƒé«˜çš„å­¦ä¹ ç‡å¯èƒ½ä½¿æ¨¡å‹åœ¨æ”¶æ•›è¿‡ç¨‹ä¸­è·³è¿‡æœ€ä¼˜ç‚¹ï¼Œè€Œè¾ƒä½çš„å­¦ä¹ ç‡åˆ™å¯èƒ½ä½¿æ¨¡å‹æ”¶æ•›é€Ÿåº¦è¿‡æ…¢ã€‚
- **è§£å†³æ–¹æ¡ˆ**ï¼š
  - **å­¦ä¹ ç‡è°ƒåº¦**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥é™ä½å­¦ä¹ ç‡ï¼Œå¯ä»¥ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
  - **åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´**ï¼šä½¿ç”¨æ–¹æ³•å¦‚ `ReduceLROnPlateau`ï¼Œåœ¨éªŒè¯æ€§èƒ½æ²¡æœ‰æå‡æ—¶ï¼Œè‡ªåŠ¨é™ä½å­¦ä¹ ç‡ã€‚

#### 3.3 è®­ç»ƒæ•°æ®çš„å˜åŒ–å’Œæ¨¡å‹çš„é‡æ–°é€‚åº”

- **ç°è±¡**ï¼šç»§ç»­è®­ç»ƒæ—¶ï¼ŒF1å¾—åˆ†å†æ¬¡ä¸Šå‡å¹¶è¶…è¿‡ä¹‹å‰çš„æœ€é«˜å€¼ï¼Œè¡¨æ˜æ¨¡å‹åœ¨è¿›ä¸€æ­¥è®­ç»ƒè¿‡ç¨‹ä¸­é‡æ–°é€‚åº”æ•°æ®ï¼Œæ‰¾åˆ°äº†æ›´å¥½çš„æ³›åŒ–è§£ã€‚
- **è§£é‡Š**ï¼šè¿™å¯èƒ½æ˜¯ç”±äº**æ¨¡å‹åœ¨è¿‡æ‹Ÿåˆåé‡æ–°æ‰¾åˆ°æ•°æ®çš„å…¨å±€ç‰¹å¾**ï¼Œå³æ¨¡å‹åœ¨ä¸€å®šç¨‹åº¦ä¸Šè·³å‡ºäº†å±€éƒ¨æœ€ä¼˜ï¼Œæ‰¾åˆ°äº†æ›´å¥½çš„å…¨å±€æœ€ä¼˜ã€‚

### 4. å‚è€ƒæ–‡çŒ®

1. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)* (NAACL-HLT 2019), 4171â€“4186. Association for Computational Linguistics.
2. Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C. 2016. Neural Architectures for Named Entity Recognition. In *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies* (NAACL-HLT 2016), 260â€“270. Association for Computational Linguistics.
3. Huang, Z., Xu, W., and Yu, K. 2015. Bidirectional LSTM-CRF Models for Sequence Tagging. arXiv preprint arXiv:1508.01991.
4. Ma, X., and Hovy, E. 2016. End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)* (ACL 2016), 1064â€“1074. Association for Computational Linguistics.
5. Akbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., and Vollgraf, R. 2019. FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)* (NAACL-HLT 2019), 54â€“59. Association for Computational Linguistics.
6. Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep Contextualized Word Representations. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)* (NAACL-HLT 2018), 2227â€“2237. Association for Computational Linguistics.
7. Akbik, A., Blythe, D., and Vollgraf, R. 2018. Contextual String Embeddings for Sequence Labeling. In *Proceedings of the 27th International Conference on Computational Linguistics* (COLING 2018), 1638â€“1649. Association for Computational Linguistics.
8. Chiu, J. P. C., and Nichols, E. 2016. Named Entity Recognition with Bidirectional LSTM-CNNs. *Transactions of the Association for Computational Linguistics*, 4, 357â€“370.
9. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. 2011. Natural Language Processing (Almost) from Scratch. *Journal of Machine Learning Research*, 12, 2493â€“2537.
10. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.

å¤§æ¨¡å‹è¾…åŠ©æƒ…å†µï¼šä»…ç”¨äºå¸®åŠ©å®šä½æŠ¥é”™åŸå› å’Œæä¾›æŠ¥å‘Šæ¡†æ¶